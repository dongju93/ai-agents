# Dongju Lee

**Backend Python Software Engineer**  
üìß spdlqj011@gmail.com | üì± +82-10-3247-8760  
üåê Portfolio: https://www.notion.so/dongjulee/Portfolio-e0c6d62ecfba4c3996a920c69322afd3 | GitHub: github.com/dongju93  
üìç Seoul, South Korea

---

## Professional Summary

Passionate Backend Developer with nearly 5 years of experience designing, developing, and maintaining Python-based services. Expert in FastAPI, Redis, PostgreSQL, and Elasticsearch integrations. Skilled in containerization and orchestration (Docker, Kubernetes) and building scalable, high-performance APIs. Proven ability to implement real-time data pipelines and automated CI/CD workflows. Eager to leverage cloud infrastructure, automation, and best practices to drive reliability and performance in AWS-powered environments.

---

## Core Technical Skills

**Programming & Frameworks:**

- Python, FastAPI, Django, RESTful API, GraphQL, gRPC, JWT Authentication

**Databases & Caching:**

- PostgreSQL, Redis, Elasticsearch, MySQL, MongoDB

**Cloud & DevOps:**

- Docker, Kubernetes, CI/CD (Jenkins), NGINX, Linux (Ubuntu), Working knowledge of AWS (EC2, RDS, S3)

**Monitoring & Logging:**

- Prometheus, Grafana, ELK Stack (Elasticsearch, Logstash, Kibana), Grafana Loki

**Data Processing & Analytics:**

- Polars, Pandas, ETL Pipelines, Real-time Streaming

---

## Professional Experience

### PINOLIKE Co., LTD.

**Backend/R&D Engineer (Assistant Manager)** | Seoul, South Korea  
_May 2024 ‚Äì Present_

- Architected and built high-performance async API servers using **FastAPI** with Pydantic, achieving <100ms response times.
- Designed a multi-database backend: **PostgreSQL** for relational data, **Redis** for caching, and **Elasticsearch** for full-text search; boosted query throughput by 40%.
- Developed async, event-driven pipelines with Redis Pub/Sub for real-time data ingestion and processing.
- Integrated Go microservices via **gRPC**, optimizing cross-language interactions.
- Implemented stateless JWT authentication with secure access/refresh token flows.
- Containerized services with **Docker** and deployed on **Kubernetes**, enabling auto-scaling and fault tolerance.
- Established CI/CD pipelines using **Jenkins**, with automated testing (Pytest), linting (Ruff), and type checks (Pyright).
- Monitored application health and logs via **Prometheus**, **Grafana**, and **ELK Stack** to ensure 99.9% uptime.

### CLUMELL Co., LTD.

**Project Manager & Technical Lead** | Seoul, South Korea  
_June 2023 ‚Äì April 2024_

- Led design and implementation of a high-throughput log collection system using **Elastic Stack** and custom Rust tooling.
- Developed a Rust-based log ingestion service, accelerating data processing by 60%.
- Authored Python scripts for endpoint IoC report extraction and transformation, processing 1,000+ JSON records daily.
- Defined REST API contracts and delivered sample code to external partners, ensuring seamless integration.
- Oversaw QA workflows and feedback loops, maintaining service reliability and performance SLAs.

### Daily Soft Co., LTD.

**IT Consultant** | Seoul, South Korea  
_January 2021 ‚Äì June 2023_

- Led cloud infrastructure design and vendor evaluation for multi-regional environments, advising on AWS solutions.
- Deployed containerized applications using **Docker** and managed PostgreSQL migrations for a national financial system.
- Designed and implemented ESB integration and database modeling for high-availability mission-critical applications.
- Developed Python-based data analysis pipelines (Pandas, Jupyter) for regulatory reporting systems.

---

## Education

**Induk University** | Bachelor of Science in Information & Communication Engineering  
March 2012 ‚Äì February 2019 | GPA: 3.54/4.5

**Seoul Technical High School** | Diploma in Information Resources  
February 2009 ‚Äì February 2012

---

## Certifications

- **Information Processing Engineer** | Korea Human Resources Development Service | May 2019

---

## Selected Personal Projects

**AI Agent Collaboration System**

- Building autonomous multi-agent workflows for news collection and summarization using Python and LLMs (CrewAI).
- Implemented orchestration, inter-agent communication, and RAG pipelines for dynamic content curation.

---
